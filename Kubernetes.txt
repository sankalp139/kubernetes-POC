Topics

Intro to K8s

K8 Architecture

Main components

Minikube and Kubectl

kubectl command

YAML conf files

Hands on Demo

Advanced concepts

K8s Ingress

Helm -- Package manager

Volumes - Persistent Data in K8

K8s Stateful Set - Deploying Stateful Apps.

Kubernetes --> Open source Container Orchestration Framework. It helps you manage applications made up of multiple containers.

What problem does it solve ?

Rise of microservice gave way to container technology thus we have an application made up of several containers. Managing these containers can be painful so container orchestration came into being.

Kubernetes feature

High Availability
Scalability
Disaster Recovery

fundamental components of Kubernetes

Nodes - Worker nodes (physical or virtual)
Pod - smallest unit in K8, abstraction over a container
pod is usually meant to run one application container or along with that a helper container but you can have multiple containers running in a pod.

Each pod gets to have it's own IP address. Pods can communicate with each other using this IP address.Pods are ephemeral i.e they die easily and a new one is created in it's place. If pods are using IP address of the pods to communicate it makes communication difficult as every time a pod dies it spins up with a new ip address and that is why we have a new component called Service.

what is a service

Service is a static ip address that is attached to each pod. The lifecycle of pod and service are not connected which means even if a pod dies service stays up and connects with the new replacement pod.

Service works for internal communication so if we want to communicate externally like hosting a webpage we would want an external service but we do not want people to know the nodeport and ip address that are being used for hosting hence we need a domain name for that we use Ingress. SO the request goes first to ingress and that sends the request to Service.


Configmap and Secret

Configmap is used to store configuration data like database urls and you can connect it to the pod directly so the pod can access the database. That way when the database name is changed you just have to modify the value in configmap without making any changes in the application pod.

But putting credentials in Configmap would not be secure, so we use secret as it stores confidential information like username and password with base64 encoding.

Volumes

when a database container or pod is restarted the data stored in it may be lost so in order to prevent the data loss we use volumes. 
 
Volumes are basically external storage attached to your pod. Thus it is not deleted when a pod is restarted and thus the data stored on it survives.
It can be a local,remote storage or cloud storage.

K8 Clusters do not manage data persistence so it is our job to make sure that there is a backup and recovery plan for the volume .

Deployment and Stateful Set

In order to prevent outage when we are in production we create replica of pods
connected to the same service. Thus, the service acts as a load balancer and sends the request to whichever pod that is less busy.
so in order to create another pod we shall use deployment.
Deployment --> blueprint for my-app pods

We do not create pods but we create deployment as it creates a set of replica for the pods.
Deployment is  a layer above pods.
so if one pod dies the request is transferred to another pod.
we shall not replicate database because database has a state it has information that gets updated every now and then and all clones need to share the common storage.

time 19:28

TO avoid data inconsistencies in data bases we use StatefulSet. Thus any database container should be created using StatefulSet and not deployment. Stateful set would take care of replication of pods and autoscaling as well as make sure that all the replicas are synchronized.

Deploying Stateful Set is not easy so people host database outside of the Kubernetes Cluster while the deployment has stateless components hosted inside the cluster.

Kubernetes Architecture

Master Node
Slave Node

Each node (cluster server) should have 3 processes running on it

1) Container Runtime Eg-Docker
2)Kubelet -- Kubernetes process that interacts with container runtime and the server. Kubelet is responsible for interacting with the Container and it's node.

Kubelet is responsible for starting a pod and then allocating it resources from the node like CPU, RAM.

3) Kube proxy -- It is responsible for forwarding requests from service

Master Node processes

1) Api Server --> When a user wants to deploy an application inside a cluster it has to interact with the Api server. It can be UI, dashboard or command Line tool. It gets the initial request for update to cluster and check for authentication. when you want to query the status of your deployment you have to make a request to the API server.

2)Scheduler --> It is responsible for starting the worker pod when the request has been made to APi server. It does not randomly select where to run the pod it has it's own way to decide it. It looks at the resource requirement for your new application and the available resources on all nodes available to decide which would be the best place to install it. It just decided just on which node the pod is going to be setup.It is actually the Kubelet that starts the pod.

3)COntroller Manager --> If pods die controller manager detects that and tries to recover the previous state as soon as possible. In order to do so he send request to the scheduler to schedule the dead pods and then scheduler decides the worker nodes like previous part.

4) Etcd --> It is a key value store of the cluster state . We can think of it as cluster brain. Any change in the cluster like creation or death of a cluster is stored by etcd key value store.

Etcd helps the api server know the health of the application running in cluster.
Etcd helps the Scheduler know which resources are available .
Etcd lets the Controller manager know about the change in the state of a pod in the cluster.

Etcd does not store actual application data.


What is minikube ?

It is a one node cluster where both master and client process run on the same machine.It can be used to test K8 on your local cluster.

Kubectl

once you have a virtual node you need to create pods and other components . For that you need to run kubectl which is a command line tool that is used to run all the pods on the node server. With Kubectl you can perform any operation related to Kubernetes cluster.

The kubectl will submit request to the API for creating the required components. Kubectl can also be used for Cloud or hybrid cluster.

minikube has kubectl as a dependency

minikube start --vm-driver=virtualbox
or
minikube start --no-vtx-check - if you face issues with hypervisor

kubectl create deployment nginx-depl --image=nginx

kubectl get pod

kubectl get replicaset

kubectl edit deployment nginx-depl --- this command creates an auto generated YAML configuration file.

kubectl logs pod-name

kubectl exec -it nginx-depl-7dc68dfbfc-tslh6 -- bin/bash

kubectl delete deployment nginx-depl

58:47

In order to do a deployment we run Kubectl apply -f (stands for file) deployment.yaml

kubectl describe pod [pod-name] -- This will get you detailed information about the pod.
1:02

configuration files are made up of three parts

Every config file has

1) metadata -- name
2) Specification -- every kind of configuration related to the component

The first two lines of the configuration file are basically the ApiVersion and the Kind of configuration that is being created with configuration file.

3) status - this will be automatically generated by Kubernetes.
using the state it checks whether the actual state is same as the desired state or not. In case both the states are not same that tells the Kubernetes and it tries to make the desired state equal to the current state and that is how the self healing mechanism of Kubernetes Cluster work.
The status data actually comes from etcd. Etcd holds the current status of the Kubernetes cluster.

Format of config files is YAML.
YAML is strict about Indentation
use online YAML validator to resolve indentation issue
You can save config files in Git repo


Layers of Abstraction in Kubernetes

Deployment
Replicaset
pod
Container

The template inside the specification section can have it's own metadata and spec section thus it is like a deployment inside the other.

Labels and Selectors

The metadata part contains labels and the specification part contains selectors.

kubectl get pod -o wide ---- will get us detailed information of a pod

 kubectl get deployment nginx-depl -o yaml > E:\Kubernetes\nginx-depl-res.yml  -- This command can help you save the configuration in a yaml file.

This yaml file can be useful when debugging as it has detailed information about the status.

kubectl delete -f nginx-depl.yaml -- this can also be used to delete an existing deployment

1:16:19

Demo project


We need to setup a mongo Db pod that only handles internal requests.

We will create a configMap and secret to store db creds and then use the secrets in deployment.yaml .

We need to create

2 Deployment/pod Mongo DB , Mongo Express
2 Service Internals service and external service
1 ConfigMap 
1 Secret

Web Reuest --> Mongo Express External service --> Mongo Express --> Mongo DB Intenal Service --->Mongo DB pod

Kubectl get all --- get all components of Kubernetes

For creating a mongo db deployment we need to figure out the image name, env variables and the port that it will listen on to add it in deployment. To setup the username and password for mongo db we will create a secret.
The secret will not contain the username and password directly. We shall do base64 encoded for the username and password by using.

echo -n 'administrator' | base64 ---- This is a bash command so it will work in WSL only

and then  save the encoded value in the secret.

To use the secret in the deployment we have to create the secret before the deployment
kubectl apply -f mongo-secret.yaml

To find detailed information on the pod you can use Kubectl describe pod <podname>

so now that we have a pod running mongodb pod the next step is to create an Internal Service that shall be used to communicate with the mongodb pod.

In YAML you can actually put multiple documents in 1 file so we can add the service in the same YAML file as Mongodb . You need to put 3 - as that means end of document.

Service Configuration File

Kind : Service
metadata/name: any random name
selector:  to connect to Pod through Label
-ports
    port: service port
    targetPort: ContainerPort of Deployment

To know more about the service after it is deployed you can use -- kubectl describe service mongodb-service

TO know IP information of a pod you can use. ---  kubectl get pod -o wide

If you want to get information about all the deployments/services related to mongodb you can use 
kubectl get all | grep mongodb


Now we will create a Mongo Express Deployment and  External Service and ConfigMap-- So that the mongodb server can be approached from outside.

ConfigMap file

Kind ConfigMap
metadata/name a random name
data  the actual contents in key-value pairs


1:40:19

The mongoexp pod is currently having issue. It is unable to find anything on the port 27017. That means either the service is not working fine or the mongo db pod does not have port mapping set up correctly.

I have
deleted mongodb and mongoexp deployment, configmap and service

The default js script running on mongo express only looks for a service named mongo. That is why it was throwing the error.(not correct)

Now the final step is to create a Mongo Express External service.

In order to create an External service we basically need to create a service of type LoadBalancer and we need to add an extra port called the NodePort that can be between 30k to 32k.

The default type for a service is Cluster IP that is why we do not define it when creating a service of type Cluster IP. When creating an internal service we can keep it of type Cluster IP as it would not need a specific IP.

The reason we are using a Loadbalancer type service is that Loadbalancer assigns it's service an Internal IP and an external IP while Cluster IP Service only assigns an Internal IP.

Usually Load balancer should automatically assign the External IP but since we are using  minikube it will not work rightaway. We need to do run minikube service mongo-express-service

issue:- I cannot login to the page that is opening.

user
administrator

The issue finally got resolved . The problem was that the default username and password for mongo express is admin:pass and that let me in.

When you create a db in mongo express uI what actually happens is that the request goes to mongo express external service which then goes to mongo express pod which then goes to mongo db Internal service which sends it to mongo db pod 

Namespaces 

A namespace is used to organize resources inside a cluster. It is like a virtual cluster inside a Cluster.

Kubernetes-dashboard namespace comeswith minikube.

kube system namespace is for system process Master and kubectl processes you should not create a resource inside this namespace.

kube public contains publicly accessible data it has a configmap that contains cluster information.

kube-node-lease it holds information about the heartbeat of node  each node has associated lease object in namespace. This determines the availability of a node.

default namespace is used for creating resources by default.

To create a namespace we can type kubectl create namespace my-namespace 

kubectl get namespaces to get the list of namespaces.

we can also create a namespace with a configuration file.

what is the need for namespaces 

if all resources are created within the same namespace it will become difficult for us to manage the resources.
we can group resources in namespace.

Eg :- dtatabase namespace monitoring namespace one for monitoring.

we can override other teams deployment by mistake.
staging and dev env can share resources using ns.
Also for blue green deployment.
Also we can limit access using namespace. only give access to specific namespace.
Also we can limit the amount of resources that each namespace can use.

you can't access most resources from another namespace for instance db pod in one ns cannot access the configmap pod in another namespace.

configmap in one ns can be accessed by db in another ns. because the db url in configmap will have url.namespace mentioned in configmap.
 you can access service from other ns. This is how you can use services like elastic search or nginx from other ns.

there are some resources that live globally and are not restricted to a ns example volume, persistent volume.
you can list resources that are not bound to a namespace using the command

kubectl api-resources --namespaced=false
same way you can list all the resources bound to a namespace using the command
kubectl api-resources --namespaced=true

kubectl commands work with default namespace 

how to create components in a namespace

kubectl apply -f abcd.yaml --namespace=my-ns

also another way is adding

namespace: my-ns in the yaml file in the metadata section.

there is another way to change default namespace to the specific namespace that we wish to use.

we can use a tool called Kubens
we can install kubens using the command brew install kubectx

type kubens and it will list all the ns with the default ns highlighted

if i type kubens my-ns
Active namespace is changed to my-ns from default.

2:01:41


Kubernetes Ingress

External service vs Ingress

when you have a url for an application you don't want it to look like http://10.24.123.50:30000

you want it to look like http://my-app.com

we can do that using ingress.

so now instead of accessing the application through the External service we create an ingress app.com that accepts the request from browser and then sends it over to the  internal service.

YAML config for External Service.

The type for this external service will be load balancer because we are opening it to public. We also mention the port number that can be accessed 

so in the YAML config for the Ingress you provide rules in which you mention the host name my-app.com and then in paths you define service name for the internal service . So basically any request made by the browser to the hostname for ingress is mapped directly to the internal service.

The type for Ingress is default so it is cluster Ip by default. Host in Ingress has to be a valid domain name. That domain name should be mapped to the IP address of the Node which will be the entry point into the cluster.

In order to create ingress just having a config file is not enough we need to setup ingress controller.

Install an ingress controller :- This is another separate pod that needs to run inside the k8 cluster this helps in implementing the ingress routing rules.

The ingress controller does the evaluation and processing of ingress rules.

Ingress controller - evaluate all the rules defined in your cluster so basically the controller will be the entry point for request and decides which rules shall be implemented.

Ingress controller
evaluates all the rules
manages redirections
entrypoint to cluster for all requests to that domain
many third party implementations ---- Kubernetes has it's own Nginx Ingress controller and there are other controllers too that we can use.


If your using AKS or EKS you would have a cloud load balancer and external request would hit this cloud load balancer and that would send the request to Ingress controller pod.
 one major advantage of using aWs or Azure  or something like that is you do not have to create your own load balancer. 

when working with bare metal you will have to setup this load balancer entrypoint yourself . 
Generally there is an external server that would act as load balancer and entrypoint into the cluster.
so the server would have public ip address and open ports.all the other nodes will not be publicly accessible. So the request passes through this load balancer and goes to the ingress controller.

in order to setup ingress controller in minikube you need to do 

minikube addons enable ingress

This automatically starts Nginx Implementation of Ingress controller.
This controller will be part of the kube-system namespace according to the video
but actually there was another namespace created ingress-nginx

2:13:24

once the ingress controller is installed we can create an ingress rule.
before setting up the ingress you would have to set up ip addess to the host.

In your /etc/hosts file you will have to setup that mapping for ip address to domain name.

Kubernetes also have a default backend.

we can also define multiple paths for Ingress. like google offer maps, drive and gmail. we can define multiple paths in paths section.
so host will be my-app.com
and in paths we shall give /abc and path /bcd or you can create multiple hosts with subdomains instead of domain.

configuring TLS certificates in Ingress.

In order to setup TLS certificate in Ingress so that we have https instead of http.
we just have to add tls above rules section and secret name that will hold the certificate in the ingress config.


the Ingress will look like 

apiVersion:
kind: 
metadata:
  name:
spec:
  tls:
  - hosts:
    - myapp.com
      secretName: myapp-secret-tls
   rules:
     - host: 
       http:
         paths:
         - path: 

and we will define a secret

apiVersion: v1
kind: Secret
metadata:
  name: maypp-secret-tls
  namespace: default
data:
  tls.crt: base64 encoded cert -- we need to put the value there
  tls.key: base64 encoded key
type: Kubernetes.io/tls

2:23:16

The secret should be in the same namespace as the Ingress component.


Helm package Manager for K8

What is Helm ?

Helm is used
as package manager for Kubernetes (like apt, yum)
It is a convenient way for packaging a collection of Kubernetes YAML files and distributing them in public and private registries.

Lets say you need to deploy elastic search in Kubernetes Cluster for logging so you need a stateful set for stateful apps like db , a configmap and secret, k8 user with permissions and some services.

creating all these YAML files manually is tedious. These files would be common for anyone who wanted to use elastic search so someone it would be nice to use the YAML files already created by another user that are packaged together . That Bundle of YAML files are called helm charts. You can create your own helm chart and push them to a helm repo or use other peoples helm repo. so it find a common chart for usual stuff.

All the DB, Monitoring apps already have helm charts available . This process of sharing charts is why helm became so famous.

you can use the command helm install chart-name command 

we can look up charts using  helm search <keyword>

or you can use helm public repo Helm hub.

There are also private registries for helm like within org. 

To install helm we can use 

curl -fsSL https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-linux-amd64 | bash -s -- -install helm

Helm is a templating engine

If you have an application with multiple microsv and deployment and service of each of these microsv are pretty much the same with difference in images tags or application names.
So we might write sepearte deployment file for each microsv without helm, but since only differenc is some values with helm you can define a common blueprint and replace the values that can change with placeholder values. This can be called template file.


The place holder .Values.container.name comes from Values.yaml file or through command line with --set flag

This can be very useful for CI/CD pipelines as you can take the template and change the value for the variables based on the pipeline before deploying.


another use of helm is deploying same set of applications across different environments.
 2:32:11

Instead of deploying individual YAML files you can create a chart with all the YAML files package them together and then use it to deploy in other environments.

heml chart structure

mychart/
chart.yaml -- contains meta information about chart like list of dependencies.
values.yaml --- place where all values are kept for the variables used in templates.These values are actually default values for those placeholders and can be updated later on.

charts/ -- this folder will have chart dependencies.
templates/ -- this folder will contain all the templates
The template files will take up values from values.YAML

how are values injected

let's say there are some values defined in values.yaml that are being used by the template files
While running the template we create and alternate my-values.yaml file  

helm install --values=my-values.yaml <chartname>

so values.yaml will be overridden by my-values.yaml so the values that were changes in my-values .yaml will get updated while the rest would remain the same.

or you can set the values directly through the command line

helm install --set version=2.0.0

Third feature of helm is release management

Helm has version 2 and 3

In version 2 the helm installation comes in two parts
Helm client(CLI) and server(Tiller)

whenever you install a helm chart using 
helm install <chartname> 
helm client sends YAML file to Tiller and Tiller will execute this request and create components inside the K8 cluster. It provides extra feature of release mgmt.

Whenever you create or change a deployment Tiller keeps a copy of the each configuration for future reference. It creates a history of chart execution. so when you do an upgrade the changes are made to the existing deployment 
helm upgrade <chartname>
instead of creating a new one. In case upgrade fails you can rollbackup using 

helm rollback <chartname>

rollback is  possible because Tiller keeps a track of all the changes requested to the deployment. 

Downside

Tiller has too much power in k8 and so now in helm 3 tiller got removed.


k8 volumes

We need a storage that doesn't depend on pod lifecycle . SO we need a storage that shall update the pod with latest info whenever it comes up.The storage should also be available to all nodes

storage needs to survive if app crashes


persistent volume -- it is a cluster resource like RAM or CPU that is used to store data. It can be created using a YAML file.

The presistent volume would still need a physical or cloud storage . K8 gives you persistent volume as an interface for your storage and you need to manage the storage yourself like taking backups and recovering them in case of issues. Storage is an external plugin to the cluster may be local or remote storage. There can be multiple persistent volumes in one cluster

2:42:41

persistent volumes are not namespaced -- there are available to entire cluster not just any namespace

Local vs Remote Volume Types
has its own use case

Local violate the 2nd and 3rd req --- not being tied to one specific node as we don't know which node will have the latest pod and also capability of surviving crash hence it is preferred to use remote storage for persistent volumes.

who creates and when ?

There are tow kind of people dealing with cluster

1) admin or Devops people
2) user or developer using pipelines to build and deploy application in K8s


Persistent volume Claim

 so admin team would be configuring the persistent volume based on requirement.
Now Developers know that volume is available but they have to setup one more component to ensure that the persistent volume can be used by their pods.


The component added for that purpose is called persistent volume claim. 

To connect the application pod to the persistent volume  we need to mention the volume claim name in the volumes section of the pod.

 Pods request the volume through Claim and the claim tries to find the persistent volume and volume will actually have the actual storage connected to it.

Although volumes are not attached to a specific namespace volume claims are and so they should be in the same namespace as the pods.


using persistent volume and volume claim seems tedious but the advantage is you as a user would not have to worry about where the volume is and where are the disks located.

There are two different volume types called

configMap
Secret

There are created differently as we already know.

A pod can use different volumes simultaneously

An elastic search app need configmap , certificate mounted as secret and a db storage like aws elastic block storage. so you list all the volumes you want to mount in the volumes section. 

It is possible that developers might need new volumes frequently as the request rises and that can be tedious for the admin to request new storage and add them on a daily basis.
This is where storage class comes into picture.

Storage class -- It provides persistent volumes dynamically. whenever pvc claims it.
2:56:25

so we define storage with provisioner value in YAML because it tells Kubernetes which provisioner to be used like AWs , GCP etc

Each storage backend has it's own provisioner

internal provisioner - "Kubernetes.io"

external provisioner -- need to find 

we can also configure parameters for storage we want to request for PV

storage class is another abstraction level that abstracts underlying storage provider as well as characteristics of storage with parameters.

In order to implement storage class we update the Persistent volume claim and add attribute storage class name that helps to link the storage class.

SO now when a pod requires storage it requests to persistent volume claim and PVC requests storage class and SC creates PV that meets the needs of the Claim using the provisioner of storage class .


stateful Set

It is a  Kubernetes component used for stateful applications.

 stateful application eg:- databases or any applications that stores data to keep track of it's state.

stateless app do not keep record of state or old transactions and each request is treated as completely new request not related to any previous request.

stateless and stateful applications are deployed differently.

stateless applications are deployed using deployment it is an abstraction of pods and helps you run duplicates.

Stateful applications are deployed using statefulset it replicated the pods of stateful apps just like deployment.

we can configure storage with both.

difference between deployment and statefulset

replicating stateful applications is more difficult and has a some other requirements as well.

lets say you have to deploy a java application and a sql database

so replicating application can be easily achieved by deployment and scaling is easier.There will be one service loadbalancer sending request to any pod for any request and deletion is done in any random order.

In case of MySQL the pod replicas can't be deleted at the same time and they cannot be accessed randomly in any order because the replica pods are not identical. They have unique identity.
Stateful Set gives each of these pods a unique identity also called sticky identity.


The pods are created from same specification but they are not interchangeable each has a persistent identifier across any re-scheduling. 

The problem with scaling db applications is that if you have one db pod you can use it for both reading and writing but if you have multiple db pods you cannot use all of them for reading and writing at the same time as that would cause inconsistencies. Thus one of the pod will be master which will be updated or used for writing while other db pods would be worker pods that would just be used for reading and they will have to be consistently synced with the master. Also these pods are not using the same physical storage they have their own storage replica. In order to achieve that they have to constantly sync the data. 

Since master is the only one updating data even in this case the worker pods are responsible for updating their storage replicas consistently. 

When a new db pod is added it clones data from the previous pods and once done it starts cloning data. The problem here is if stateful set crashes or the pods or nodes die all data is lost. That is why using persistent storage is a good idea so the data would stay intact irrespective of what happens to pods or nodes. 

Each pod has it's own state stores in the pods persistent storage so when the pod dies or gets replaced the volume gets reattached to the new pod and the pod gets to its latest state.

stateful sets get a fix ordered name like mysql-0,1,2...

The first one is master . If the first pod is not up and running stateful set will not start the second pod. It will wait so as to maintain the order. same thing happens with deletion as well. In case of deletion it will delete mysql-2 then mysql-1 and the 0.

Each pod has it's own dns endpoint so there is a loadbalancer that can address any replica pod and then each pod will have it's own dns like mysql-0.svc, mysql-1.svc

These characteristics ensure that even when pods restart the dns and service name stays the same .

Replicating statefulapp is complex 

we need to configure cloning and data sync
and also do managing and backup and recovery on your own

Kubernetes ideally are not a good idea for stateful applications it is good enough for stateless apps. 

Services

3:14:05

Each pods have their own IP address but they are ephemeral so when they restart their IP address changes.
service provides a stable static Ip address that doesn't change even when the pod dies.
It also acts as a loadbalancer between replicas of a pod.
It helps in having loose coupling as they can provide a layer of abstraction when having multiple clusters communicating with each other.

Types of services

Cluster IP -- this is the default service type . 

let's say you have a pod running a container with a microservice and a sidecar container for collecting logs both are listening on different ports the ip they will get will be from the list of ip ranges available for the node.
we have an ingress to accept requests from the browser which then goes to a service with an ip address.
The ingress is mapped to the service and we define service by name and then dns resolution  maps the service name to the IP address

how does service know which pod to forward to and which port to forward to ?

In the service config we define selector which should be same as the label defined for these pods. The pod should match all the selectors.

Target port for service will be used to send the request by service to the application pod.

whenever you create a service Kubernetes will create a service endpoint object which will have same name as service. This endpoint object will keep track which pods are endpoint of that service. whenever you create a new pod the endpoint may change so the endpoint keeps a track of those changes. The service port can be arbitrary but the targetPort must match the container port that it is listening at.
Let's say we  have a mongo db container along with sidercar container that helps Prometheus in log collection that means the service that was used for mongodb now has to deal with two different ports one for the app and one for prometheus.
In this case you have to name the ports.

Headless service

If a client wants to communicate  specific pod or the pods need to communicate amongst themselves the service would be useless for them. This happens in case of stateful applications where each pod is unique.

In order to connect specifically to a pod we can send a request to K8 API and it will return the list of pods and their Ip addresses. But this will make the app too tied to the specific API and also make it inefficient. K8 allows client to discover pod Ips through DNS lookup so when we do DNS lookup for a service it return the IP address of the specific service.
But if you set the ClusterIP to none for the service then the DNS lookup will return the pod Ip address instead of the server IP address.

so a headless service will have the cluster IP to none so it will not have a cluster IP
we usually use it for stateful apps , sometimes there is a clusterIP service along with a headless service.So to reach all the pods we use the clusterIP service while to reach out to a specific pod we use the headless service.

There are basically three types of attributes that yo can use for type in service 
ClusterIP
NodePort
LoadBalancer

A Nodeport service has a static port number and unlike clusterIP service which is accessible only within the cluster nodeport service is open to external traffic.The port is defined using the nodePort parameter and the range for nodeport is between 30000 - 32767


so we can use ip address of the worker node and the nodeport for communication from outside. whenever we create a nodeport service a clusterIP service to which the node port service will be routed is automatically created. Service will expand over all of the worker nodes.

Nodeport is not very secure as this can make the pods vulnerable as nodeport can talk to a pod on any node

so we use Loadbalancer service type -- This is a service provided by the cloud provider and we do not have to create it . Each cloud provider has it's own native loadbalancer . That is created and used whenever we create a loadbalancer service nodeport and clusterIP service is created automatically. so the config file for loadbalancer will now have a port of the cluster IP and a nodeport for the nodeport service created. Both cluster IP and nodeport will not be accessible outside only the loadbalancer will be accessible outside.


 
To get pods in a specific namespace

get pods -n <namespace>

helm command to install Prometheus with chart 